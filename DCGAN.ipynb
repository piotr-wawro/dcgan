{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16021,"status":"ok","timestamp":1652031193890,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"e7pP2neRf_d5","outputId":"74c5ce69-64df-4cec-f55d-fa91114b1e03"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2984,"status":"ok","timestamp":1652031198185,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"s2sqis-pm-MQ"},"outputs":[],"source":["import time\n","import os\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1652031200398,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"Gpuq4XAYhLdl"},"outputs":[],"source":["BASE_PATH = '.'\n","BUFFER_SIZE = 60000\n","BATCH_SIZE = 256\n","EPOCHS = 30\n","NOISE_DIM = 100\n","SEED = tf.random.normal([16, NOISE_DIM])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":583,"status":"ok","timestamp":1652031202619,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"edoQuxmWmhYk","outputId":"050ae79f-2b83-4d86-81cb-c7628e52aacb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 12544)             1254400   \n","                                                                 \n"," batch_normalization (BatchN  (None, 12544)            50176     \n"," ormalization)                                                   \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n","                                                                 \n"," reshape (Reshape)           (None, 7, 7, 256)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n"," nspose)                                                         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n"," ranspose)                                                       \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1600      \n"," ranspose)                                                       \n","                                                                 \n","=================================================================\n","Total params: 2,330,944\n","Trainable params: 2,305,472\n","Non-trainable params: 25,472\n","_________________________________________________________________\n"]}],"source":["def make_generator_model():\n","    model = tf.keras.Sequential()\n","\n","    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(NOISE_DIM,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Reshape((7, 7, 256)))\n","\n","    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n","\n","    return model\n","\n","generator = make_generator_model()\n","generator.summary()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":237,"status":"ok","timestamp":1652031204842,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"8AG-3cqGnD3B","outputId":"d1afe914-19f7-4fc3-bb97-c2a4e52e16a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n","                                                                 \n"," dropout (Dropout)           (None, 14, 14, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 7, 7, 128)         0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 6273      \n","                                                                 \n","=================================================================\n","Total params: 212,865\n","Trainable params: 212,865\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["def make_discriminator_model():\n","    model = tf.keras.Sequential()\n","    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(1))\n","\n","    return model\n","\n","discriminator = make_discriminator_model()\n","discriminator.summary()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":243,"status":"ok","timestamp":1652031206871,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"bvw-ZSexr7U6"},"outputs":[],"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652031208315,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"7kmOCc8nr2aQ"},"outputs":[],"source":["def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652031209439,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"98WfmhpLsDvo"},"outputs":[],"source":["def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652031210237,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"pigBCttwsEZl"},"outputs":[],"source":["generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652031211369,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"pRh_JTAosTY0","outputId":"8f332431-60f2-41a9-8651-737448cb3121"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing from scratch.\n"]}],"source":["checkpoint_dir = BASE_PATH + '/training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)\n","ckptManager = tf.train.CheckpointManager(checkpoint, 'checkpoint_prefix', max_to_keep=10)\n","\n","checkpoint.restore(ckptManager.latest_checkpoint)\n","if ckptManager.latest_checkpoint:\n","  print(\"Restored from {}\".format(ckptManager.latest_checkpoint))\n","else:\n","  print(\"Initializing from scratch.\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1652031216245,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"r-9nrWwsuomw"},"outputs":[],"source":["def generate_and_save_images(model, epoch, test_input):\n","    predictions = model(test_input, training=False)\n","\n","    fig = plt.figure(figsize=(4, 4))\n","\n","    for i in range(predictions.shape[0]):\n","        plt.subplot(4, 4, i+1)\n","        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n","        plt.axis('off')\n","\n","    plt.savefig(f'image_at_epoch_{epoch}.png')\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652031217226,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"IIyo7ejPEV2F"},"outputs":[],"source":["@tf.function\n","def train_step(images):\n","    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_images = generator(noise, training=True)\n","\n","        real_output = discriminator(images, training=True)\n","        fake_output = discriminator(generated_images, training=True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["@tf.function\n","def train_discriminator(train_dataset):\n","    while True:\n","        output_real = np.array([])\n","        output_fake = np.array([])\n","\n","        for image_batch in train_dataset:\n","            noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n","\n","            with tf.GradientTape() as disc_tape:\n","                generated_images = generator(noise, training=False)\n","\n","                real_output = discriminator(image_batch, training=True)\n","                fake_output = discriminator(generated_images, training=True)\n","                np.concatenate((output_real,real_output))\n","                np.concatenate((output_fake,fake_output))\n","\n","                disc_loss = discriminator_loss(real_output, fake_output)\n","\n","            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","            discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","\n","        train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n","        train_acc_metric.update_state(output_real, tf.ones_like(output_real))\n","        train_acc_metric.update_state(fake_output, tf.zeros_like(fake_output))\n","        if train_acc_metric.result() > 75:\n","            break"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["@tf.function\n","def train_generator(train_dataset):\n","    while True:\n","        output = np.array([])\n","\n","        for image_batch in train_dataset:\n","            noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n","\n","            with tf.GradientTape() as gen_tape:\n","                generated_images = generator(noise, training=True)\n","\n","                fake_output = discriminator(generated_images, training=True)\n","                np.concatenate((output,fake_output))\n","\n","                gen_loss = generator_loss(fake_output)\n","\n","            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","            generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","\n","        train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n","        train_acc_metric.update_state(output, tf.ones_like(output))\n","        if train_acc_metric.result() > 75:\n","            break"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":233,"status":"ok","timestamp":1652031249994,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"jYXLC0QO-MOQ"},"outputs":[],"source":["def train(train_dataset, epochs):\n","  for epoch in range(epochs):\n","    start = time.time()\n","\n","    train_discriminator(train_dataset)\n","    train_generator(train_dataset)\n","\n","      # if (batch + 1) % 50 == 0:\n","        # print (f'Finished batch {batch + 1} (size = {BATCH_SIZE}): {time.time()-start} sec')\n","\n","    generate_and_save_images(generator, int(checkpoint.save_counter), SEED)\n","\n","    # Save the model every 1 epoch\n","    if (epoch + 1) % 1 == 0:\n","      ckptManager.save()\n","\n","    print (f'Time for epoch {epoch + 1} is {time.time()-start} sec')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1652031252518,"user":{"displayName":"Piotr Wawro","userId":"00256984746504804507"},"user_tz":-120},"id":"lBNltFik_jH2","outputId":"5ee42c24-21c4-40fc-e4c3-a95dd81f05ff"},"outputs":[],"source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"16AUOmxXCQMP"},"outputs":[{"ename":"NotImplementedError","evalue":"in user code:\n\n    File \"C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_2428\\3320939595.py\", line 17, in train_discriminator  *\n        np.concatenate((output_real,real_output))\n    File \"<__array_function__ internals>\", line 180, in concatenate  **\n        \n\n    NotImplementedError: Cannot convert a symbolic Tensor (sequential_1/dense_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32me:\\dcgan\\DCGAN.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000016?line=0'>1</a>\u001b[0m train(train_dataset, EPOCHS)\n","\u001b[1;32me:\\dcgan\\DCGAN.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_dataset, epochs)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=2'>3</a>\u001b[0m   start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=4'>5</a>\u001b[0m   train_discriminator(train_dataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=5'>6</a>\u001b[0m   train_generator(train_dataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=7'>8</a>\u001b[0m     \u001b[39m# if (batch + 1) % 50 == 0:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/dcgan/DCGAN.ipynb#ch0000014?line=8'>9</a>\u001b[0m       \u001b[39m# print (f'Finished batch {batch + 1} (size = {BATCH_SIZE}): {time.time()-start} sec')\u001b[39;00m\n","File \u001b[1;32me:\\dcgan\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32me:\\dcgan\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///e%3A/dcgan/venv/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n","\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\piotr\\AppData\\Local\\Temp\\ipykernel_2428\\3320939595.py\", line 17, in train_discriminator  *\n        np.concatenate((output_real,real_output))\n    File \"<__array_function__ internals>\", line 180, in concatenate  **\n        \n\n    NotImplementedError: Cannot convert a symbolic Tensor (sequential_1/dense_1/BiasAdd:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"]}],"source":["train(train_dataset, EPOCHS)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"DCGAN.ipynb","provenance":[]},"interpreter":{"hash":"c1080945a67b3e1e0eca6e9e436b63f3381c12303c6b2cc43db74709abeef6c8"},"kernelspec":{"display_name":"Python 3.10.2 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":0}
